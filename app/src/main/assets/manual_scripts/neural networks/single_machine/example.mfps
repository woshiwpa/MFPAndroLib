
#This is an example for artificial neural network. It runs in a single machine
#这是人工神经网的例子。它在单一设备上运行。

@execution_entry mfpexample ::single_machine:: run_ANN

citingspace ::mfpexample
citingspace single_machine
function run_ANN()
	variable sample = generate_data()
	variable X = sample[0], y = sample[1]
	variable trainData0Cnt = 0, trainData1Cnt = 0
	variable idx0 = 0, idx1 = 0
	for variable idx = 0 to size(y)[0] - 1 step 1
		if y[idx] == 0
			trainData0Cnt = trainData0Cnt + 1
		else
			trainData1Cnt = trainData1Cnt + 1
		endif
	next
	
	variable trainData0 = alloc_array(trainData0Cnt), trainData1 = alloc_array(trainData1Cnt)
	idx0 = 0
	idx1 = 0
	for variable idx = 0 to size(y)[0] - 1 step 1
		if y[idx] == 0
			trainData0[idx0] = X[idx]
			idx0 = idx0 + 1
		else
			trainData1[idx1] = X[idx]
			idx1 = idx1 + 1
		endif
	next
	
	variable model = train_ANN(X, y)
	
	variable xCoordMax = -100000000.0, xCoordMin = 100000000.0, yCoordMax = -100000000.0, yCoordMin = 100000000.0
	for variable idx = 0 to size(X)[0] - 1 step 1
		if xCoordMax < X[idx, 0]
			xCoordMax = X[idx, 0]
		endif
		if xCoordMin > X[idx, 0]
			xCoordMin = X[idx, 0]
		endif
		if yCoordMax < X[idx, 1]
			yCoordMax = X[idx, 1]
		endif
		if yCoordMin > X[idx, 1]
			yCoordMin = X[idx, 1]
		endif
	next
	variable h = 0.1
	variable xSize = ceil((xCoordMax - xCoordMin) / h + 1)
	variable ySize = ceil((yCoordMax - yCoordMin) / h + 1)
	variable testData = alloc_array(xSize * ySize)
	for variable xIdx = 0 to xSize - 1 step 1
		for variable yIdx = 0 to ySize - 1 step 1
			testData[xIdx * ySize + yIdx] = [xCoordMin + xIdx * h, yCoordMin + yIdx * h]
		next
	next
	variable testDataOutput = predict(model[0], model[1], model[2], model[3], testData)
	variable testData0Cnt = 0, testData1Cnt = 0
	for variable idx = 0 to size(testDataOutput)[0] - 1 step 1
		if testDataOutput[idx] == 0
			testData0Cnt = testData0Cnt + 1
		else
			testData1Cnt = testData1Cnt + 1
		endif
	next
	
	variable testData0 = alloc_array(testData0Cnt), testData1 = alloc_array(testData1Cnt)
	idx0 = 0
	idx1 = 0
	for variable idx = 0 to size(testDataOutput)[0] - 1 step 1
		if testDataOutput[idx] == 0
			testData0[idx0] = testData[idx]
			idx0 = idx0 + 1
		else
			testData1[idx1] = testData[idx]
			idx1 = idx1 + 1
		endif
	next
	
	variable width = xCoordMax - xCoordMin, height = yCoordMax - yCoordMin
	plot_multi_xy("ANN_test_result", "chart_type:multiXY;chart_title:ANN test result;x_title:x;x_min:" + (xCoordMin - width/3) + ";x_max:" + (xCoordMax + width/3) + ";x_labels:6;y_title:y;y_min:" + (yCoordMin - height/3) + ";y_max:" + (yCoordMax + height/3) + ";y_labels:5;background_color:black;show_grid:false", _
	"curve_label:train 0;point_color:blue;point_style:x;point_size:5;line_color:blue;line_style:solid;line_size:0", trainData0'[0], trainData0'[1], _
	"curve_label:train 1;point_color:red;point_style:x;point_size:5;line_color:red;line_style:solid;line_size:0", trainData1'[0], trainData1'[1], _
	"curve_label:test 0;point_color:green;point_style:circle;point_size:5;line_color:green;line_style:solid;line_size:0", testData0'[0], testData0'[1], _
	"curve_label:test 1;point_color:magenta;point_style:circle;point_size:5;line_color:magenta;line_style:solid;line_size:0", testData1'[0], testData1'[1])
endf

function master_mind(W1, b1, W2, b2)
	// here we define ANN topology. A node in the topology chart is a perceptron.
	// A node is defined like [[[0,0.5],[3,1.3],[2,1]], 0.3, [4,8]], where [[0,0.5],
	// [3,1.3],[2,1]] is a list of input nodes and corresponding input weights. 0.3
	// is the bias adjustment. [4,8] is a list of output nodes. if a node number is
	// negative, it means input/output is from external。
	// 这里我们定义人工神经网的拓扑图。在拓扑图中一个节点就是一个神经元。每个节
	// 点的定义类似于[[[0,0.5],[3,1.3],[2,1]], 0.3, [4,8]]。这里[[0,0.5],[3,1.3],
	// [2,1]]是输入节点和对应的信号强度。0.3是偏向矫正。[4,8]是输出节点。如果一个
	// 节点的值是负值，意味着从外部环境输入/输出。
	variable n0 = [[[-1, 1.0]], 0, [2, 3, 4]], n1 = [[[-1, 1.0]], 0, [2, 3, 4]]
	variable n2 = [[[0, W1[0,0]], [1, W1[1,0]]], b1[0,0], [5, 6]], _
			n3 = [[[0, W1[0,1]], [1, W1[1,1]]], b1[0,1], [5, 6]], _
			n4 = [[[0, W1[0,2]], [1, W1[1,2]]], b1[0,2], [5, 6]]
	variable n5 = [[[2, W2[0,0]], [3, W2[1,0]], [4, W2[2,0]]], b2[0,0], [-1]], n6 = [[[2, W2[0,1]], [3, W2[1,1]], [4, W2[2,1]]], b2[0,1], [-1]]
	return [n0, n1, n2, n3, n4, n5, n6]
endf

// perceptron function
// perceptron函数
function perceptron(node, inputs, funcType)
	variable minInputCnt = min(size(node[0])[0], size(inputs)[0])
	variable idx, sumOfWeightedInput = 0
	for idx = 0 to minInputCnt - 1 step 1
		sumOfWeightedInput = sumOfWeightedInput + node[0][idx][1] * inputs[idx]
	next
	variable output = sumOfWeightedInput + node[1]
	if funcType == 1
		variable expOutput = exp(Output), expMinusOutput = exp(-Output)
		output = (expOutput - expMinusOutput)/(expOutput + expMinusOutput)
	elseif funcType == 2
		output = exp(output)
	endif
	return output
endf

// Helper function to evaluate the total loss on the dataset
// 计算数据集的总误差。
function calculate_loss(allNodes, X, y, regLambda, W1, W2)
	variable dataLoss = 0
	for variable idx = 0 to size(X)[0] - 1 step 1
		// forward propagation
		variable outputs = alloc_array(size(allNodes)[0])
		outputs[0] = perceptron(allNodes[0], [X[idx][0]], 0)
		outputs[1] = perceptron(allNodes[1], [X[idx][1]], 0)
		outputs[2] = perceptron(allNodes[2], [outputs[0], outputs[1]], 1)
		outputs[3] = perceptron(allNodes[3], [outputs[0], outputs[1]], 1)
		outputs[4] = perceptron(allNodes[4], [outputs[0], outputs[1]], 1)
		// output[5] and output[6] are both positive
		outputs[5] = perceptron(allNodes[5], [outputs[2], outputs[3], outputs[4]], 2)
		outputs[6] = perceptron(allNodes[6], [outputs[2], outputs[3], outputs[4]], 2)
		variable probs = [[outputs[5]/(outputs[5]+outputs[6]), outputs[6]/(outputs[5]+outputs[6])]]
		
		// calculate the loss.计算误差
		variable corectLogProbs = -log(probs[0, y[idx]])
		dataLoss = dataLoss + corectLogProbs
	next
	// add regulatization term to loss (optional). 将regulatization加入误差（可做可不做）
	variable weightSqr = 0
	for variable idx = 0 to size(W1)[0] - 1 step 1
		for variable idx1 = 0 to size(W1)[1] - 1 step 1
			weightSqr = weightSqr + W1[idx, idx1] ** 2
		next
	next
	for variable idx = 0 to size(W2)[0] - 1 step 1
		for variable idx1 = 0 to size(W2)[1] - 1 step 1
			weightSqr = weightSqr + W2[idx, idx1] ** 2
		next
	next
	dataLoss = dataLoss + regLambda / 2 * weightSqr
	return dataLoss / size(X)[0]
endf

function predict(W1, b1, W2, b2, X)
	variable allNodes = master_mind(W1, b1, W2, b2)
	variable y = alloc_array(size(X)[0], 0)
	// not use matrix calculation. 不用Matrix计算
	for variable idx = 0 to size(X)[0] - 1 step 1
		// forward propagation
		variable outputs = alloc_array(size(allNodes)[0])
		outputs[0] = perceptron(allNodes[0], [X[idx][0]], 0)
		outputs[1] = perceptron(allNodes[1], [X[idx][1]], 0)
		outputs[2] = perceptron(allNodes[2], [outputs[0], outputs[1]], 1)
		outputs[3] = perceptron(allNodes[3], [outputs[0], outputs[1]], 1)
		outputs[4] = perceptron(allNodes[4], [outputs[0], outputs[1]], 1)
		// output[5] and output[6] are both positive
		outputs[5] = perceptron(allNodes[5], [outputs[2], outputs[3], outputs[4]], 2)
		outputs[6] = perceptron(allNodes[6], [outputs[2], outputs[3], outputs[4]], 2)
		
		if outputs[5] >= outputs[6]
			y[idx] = 0
		else
			y[idx] = 1
		endif
	next
	return y
endf

function train_ANN(X, y)
	variable W1 = [[1.24737338, 0.28295388, 0.69207227], [1.58455078, 1.32056292, -0.69103982]]
	variable b1 = [[0, 0, 0]]
	variable W2 = [[0.5485338, -0.08738612], [-0.05959343, 0.23705916], [0.08316359, 0.8396252]]
	variable b2 = [[0, 0]]
	variable epsilon = 0.01, regLambda = 0.01
	
	for variable numPasses = 0 to 198 step 1
		// create ANN
		// 创建人工神经网
		variable allNodes = master_mind(W1, b1, W2, b2)
		variable dW2 = [[0, 0], [0, 0], [0, 0]], db2 = [[0, 0]], dW1 = [[0, 0, 0], [0, 0, 0]], db1 = [[0, 0, 0]]
		// not use matrix calculation. 不用Matrix计算
		for variable idx = 0 to size(X)[0] - 1 step 1
			// forward propagation
			variable outputs = alloc_array(size(allNodes)[0])
			outputs[0] = perceptron(allNodes[0], [X[idx][0]], 0)
			outputs[1] = perceptron(allNodes[1], [X[idx][1]], 0)
			outputs[2] = perceptron(allNodes[2], [outputs[0], outputs[1]], 1)
			outputs[3] = perceptron(allNodes[3], [outputs[0], outputs[1]], 1)
			outputs[4] = perceptron(allNodes[4], [outputs[0], outputs[1]], 1)
			// output[5] and output[6] are both positive
			outputs[5] = perceptron(allNodes[5], [outputs[2], outputs[3], outputs[4]], 2)
			outputs[6] = perceptron(allNodes[6], [outputs[2], outputs[3], outputs[4]], 2)
			
			// back-propagation
			variable delta3 = [[outputs[5]/(outputs[5]+outputs[6]), outputs[6]/(outputs[5]+outputs[6])]]
			delta3[0, y[idx]] = delta3[0, y[idx]] - 1
			dW2 = dW2 + [[outputs[2]], [outputs[3]], [outputs[4]]] * delta3
			db2 = db2 + delta3
			variable delta2 = delta3 * W2'
			delta2[0, 0] = delta2[0, 0] * (1 - outputs[2] **2)
			delta2[0, 1] = delta2[0, 1] * (1 - outputs[3] **2)
			delta2[0, 2] = delta2[0, 2] * (1 - outputs[4] **2)
			dW1 = dW1 + [[outputs[0]], [outputs[1]]] * delta2
			db1 = db1 + delta2
		next
		
		// add regularization terms (b1 and b2 don't have regularization terms)
		// 加入regularization（只有W1和W2有）
		dW1 = dW1 + regLambda * W1
		dW2 = dW2 + regLambda * W2
		
		// gradient descent parameter update
		//根据梯度更新参数
		W1 = W1 - epsilon * dW1
		b1 = b1 - epsilon * db1
		W2 = W2 - epsilon * dW2
		b2 = b2 - epsilon * db2
		
		//if mod(numPasses, 10) == 0
			print_line("Loss after iteration " + numPasses + " : " + calculate_loss(allNodes, X, y, regLambda, W1, W2))
		//endif
	next
	return [W1, b1, W2, b2]
endf

function generate_data()
	// create training inputs and target outputs.
	// 创建训练输入和目标输出。
	variable X =  [[ 0.74346118,  0.46465633], _
				   [ 1.65755662, -0.63203157], _
				   [-0.15878875,  0.25584465], _
				   [-1.088752  , -0.39694315], _
				   [ 1.768052  , -0.25443213], _
				   [ 1.95416454, -0.12850579], _
				   [ 0.93694537,  0.36597075], _
				   [ 0.88446589, -0.47595401], _
				   [ 0.80950246,  0.3505231 ], _
				   [ 1.2278091 , -0.64785108], _
				   [-0.38454276,  0.50916381], _
				   [ 0.09252135, -0.31618454], _
				   [ 1.79531658, -0.32235591], _
				   [ 1.43861749, -0.15796611], _
				   [-0.82364866,  0.86822754], _
				   [ 0.99633397,  0.1731019 ], _
				   [ 0.66388701,  0.94659669], _
				   [ 0.13229471, -0.26032619], _
				   [ 0.2482245 ,  0.7860477 ], _
				   [-1.00392102,  1.15207238], _
				   [ 2.08208438,  0.00715606], _
				   [ 0.87081342, -0.4366643 ], _
				   [ 0.37268327,  1.01743002], _
				   [ 1.26735927, -0.11813675], _
				   [-0.13270154,  1.26653562], _
				   [ 0.20331   ,  0.19519454], _
				   [ 1.98373996, -0.11222315], _
				   [ 1.82749513, -0.03085446], _
				   [-0.03857867,  0.0838378 ], _
				   [ 0.03351023,  0.63113817], _
				   [ 0.94193283,  0.63204507], _
				   [-0.39131894,  0.40925201], _
				   [ 0.88357043, -0.35868845], _
				   [-0.01141219,  0.30437635], _
				   [ 0.75877114,  0.76057045], _
				   [ 1.79414416,  0.28323389], _
				   [ 0.56116634, -0.0330033 ], _
				   [ 0.87161309,  0.01715969], _
				   [-0.75191922,  0.63798317], _
				   [-0.21911253,  0.49662864], _
				   [ 0.63711933, -0.55537183], _
				   [-0.25531442,  0.83953933], _
				   [ 0.57753017,  0.64564015], _
				   [ 0.15931878, -0.02835184], _
				   [ 1.53296943, -0.36277826], _
				   [-0.24648981,  1.09136047], _
				   [ 1.16443301,  0.01495781], _
				   [-0.70574528,  0.54883003], _
				   [ 0.16919147, -0.30895665], _
				   [ 1.0717818 , -0.40141988], _
				   [-0.8970433 ,  0.87690996], _
				   [ 0.4828491 , -0.21452374], _
				   [ 2.25536302,  0.02862685], _
				   [-0.62523133,  0.03868576], _
				   [ 1.22821377, -0.50119159], _
				   [ 0.84248307,  0.55728315], _
				   [ 0.45857236,  0.5017019 ], _
				   [ 0.98031957, -0.56811367], _
				   [ 0.1059936 ,  0.90514125], _
				   [-0.21582418,  1.03521642], _
				   [ 0.06721632, -0.1649077 ], _
				   [-1.07873435,  0.36644163], _
				   [ 1.60172165, -0.37604995], _
				   [ 1.02592325,  0.42143427], _
				   [ 1.06739115, -0.38783511], _
				   [-1.35462041,  0.28524762], _
				   [-0.20784982,  1.09043495], _
				   [ 1.61652485, -0.29469483], _
				   [ 0.26375409,  0.91508367], _
				   [-0.99805184,  0.62420544], _
				   [ 0.62273618, -0.52804644], _
				   [-1.0873102 ,  0.78128608], _
				   [ 0.01262924, -0.59715374], _
				   [-0.52953439,  0.69307316], _
				   [ 0.78362442, -0.25844144], _
				   [-0.94262451,  0.57258351], _
				   [ 0.09048712,  0.0890939 ], _
				   [ 0.99716574,  0.35017425], _
				   [ 0.4630177 ,  0.86392418], _
				   [ 0.71787709, -0.09708361], _
				   [ 2.13330659,  0.11200406], _
				   [-0.41467068,  0.92254691], _
				   [ 0.6233932 , -0.69422694], _
				   [ 2.04970274,  0.66368306], _
				   [-0.00353234,  0.21487064], _
				   [-0.27631969,  1.34161045], _
				   [ 0.82262609, -0.02317445], _
				   [-0.46610015,  0.98764879], _
				   [ 0.64426474, -0.36209808], _
				   [ 1.96682571,  0.2646737 ], _
				   [ 0.71060915,  0.80990546], _
				   [ 1.12820353,  0.4664342 ], _
				   [ 1.99150162,  0.02534858], _
				   [-0.66342048,  0.85301441], _
				   [ 2.0436285 ,  0.24563453], _
				   [ 1.77377397, -0.10513907], _
				   [ 1.773464  , -0.34102513], _
				   [ 0.66137686, -0.31314104], _
				   [-1.15442774,  0.40574243], _
				   [ 0.04167562, -0.07462092], _
				   [ 1.40426435, -0.93206382], _
				   [ 1.99317676,  0.48903983], _
				   [ 0.17673342,  1.3178874 ], _
				   [ 1.12344625, -0.09556327], _
				   [-0.64018301,  0.75214137], _
				   [ 0.17295579,  0.60135526], _
				   [-0.97644617,  0.03612864], _
				   [-0.56357758,  1.15774717], _
				   [ 1.60440089, -0.35116358], _
				   [ 0.13387667,  0.6944329 ], _
				   [-0.59909677,  0.76903039], _
				   [ 0.1023533 ,  1.09326207], _
				   [-0.22047436,  1.28343927], _
				   [-0.70416708,  0.30649274], _
				   [ 0.95709601,  0.30502143], _
				   [ 1.65936346, -0.70351567], _
				   [ 0.18911691,  0.64887424], _
				   [ 2.02773677,  0.25021451], _
				   [ 0.6515764 , -0.40677494], _
				   [ 0.55688998,  0.26120887], _
				   [ 0.81816111,  0.78952806], _
				   [-0.48367053,  0.43679813], _
				   [-0.14739828,  0.22556193], _
				   [ 0.11834786,  0.99156023], _
				   [-0.25253387,  0.18776697], _
				   [-0.93313522,  0.73385959], _
				   [ 0.6975216 , -0.11832611], _
				   [ 0.33332321,  0.14006592], _
				   [ 1.06519327, -0.38867949], _
				   [ 1.9369961 ,  0.63112161], _
				   [ 1.05840957,  0.51858443], _
				   [-0.50840939,  0.55259494], _
				   [-1.32109805,  0.51437657], _
				   [ 0.29449971, -0.26078938], _
				   [ 1.33653621, -0.18005761], _
				   [ 1.51241178,  0.11081331], _
				   [ 1.01934807, -0.17993629], _
				   [-1.13305483,  0.11109962], _
				   [ 2.07463826,  0.51253705], _
				   [ 0.73451679,  0.5346233 ], _
				   [-0.12213442,  0.15292037], _
				   [-0.0557186 ,  0.57286794], _
				   [ 0.45046033,  1.09585861], _
				   [-0.7204608 ,  1.01733354], _
				   [-0.33698825,  0.89060661], _
				   [ 1.0628775 ,  0.17231496], _
				   [ 0.34005355,  0.32486358], _
				   [ 1.24491552, -0.5137574 ], _
				   [ 0.30966003,  1.16677531], _
				   [-0.06114159, -0.02921072], _
				   [ 0.48281721, -0.43196099], _
				   [ 1.68734249, -0.6872367 ], _
				   [ 0.80862106,  0.28415372], _
				   [ 0.29809162,  0.82211432], _
				   [ 0.8496547 , -0.30507345], _
				   [-0.3802171 ,  0.88414623], _
				   [ 1.32734432, -0.48056888], _
				   [ 0.23337057,  0.10750568], _
				   [ 0.68841773,  1.15068264], _
				   [ 0.6779624 ,  0.78024482], _
				   [ 0.3395913 , -0.02223857], _
				   [ 1.30440877, -0.52950917], _
				   [ 0.75307594,  0.8526869 ], _
				   [ 1.4298847 , -0.21080222], _
				   [ 0.55631903, -0.70781481], _
				   [ 1.45384401,  0.12718529], _
				   [ 0.3203754 ,  0.87271389], _
				   [ 0.53148147, -0.27424077], _
				   [ 1.51658699, -0.45069719], _
				   [ 0.99826403, -0.80979075], _
				   [ 0.63918299,  0.96606739], _
				   [-1.2855903 ,  0.10677262], _
				   [-1.07840959,  0.56402523], _
				   [-0.57716798,  0.2942259 ], _
				   [ 0.25403599, -0.00644002], _
				   [ 0.91722632, -0.29657499], _
				   [ 1.43380709,  0.69183071], _
				   [-0.70851168,  0.49617855], _
				   [-0.64683386,  0.46971252], _
				   [ 0.30143461,  0.76398572], _
				   [ 1.48069489, -0.3572808 ], _
				   [-1.02663961,  0.41265823], _
				   [ 1.89660871,  0.25413209], _
				   [ 2.04251223, -0.46074593], _
				   [ 1.92673019,  0.40817963], _
				   [ 0.35766276,  1.0872811 ], _
				   [ 0.1240315 ,  0.67672995], _
				   [ 0.97332087, -0.70530678], _
				   [-0.72894228,  0.44179419], _
				   [-0.69863061,  0.77620293], _
				   [-0.93516752,  0.43520803], _
				   [ 0.45166927,  1.00185497], _
				   [ 0.87629641,  0.28951999], _
				   [ 0.88155818,  0.23925957], _
				   [-0.07795147,  0.27995261], _
				   [-0.56365899,  0.8918972 ], _
				   [ 1.6049806 ,  0.13835516], _
				   [ 0.27695668,  0.01210816], _
				   [ 0.25919429,  1.04104213], _
				   [ 1.5215205 , -0.1258923 ]]
	variable y =  [0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, _
				   0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, _
				   1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, _
				   0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, _
				   1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, _
				   0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, _
				   0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, _
				   1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, _
				   1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, _
				   0, 1]
	return [X, y]
endf
endcs
endcs
